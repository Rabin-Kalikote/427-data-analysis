---
title: "Ask Mattrab Note-Views Analysis" 
author: "Rabin Kumar Kalikote"
editor: visual
format:
  html:
    embed-resources: true
---

```{r setup, include=FALSE}
# load packages here
library(tidyverse)
library(tidymodels)
library(ISLR2)
library(ggplot2)
library(broom)
library(dplyr)
library(tidyr)
library(stringr)
library(forcats)
library(knitr)
library(ggrepel)
library(datasets)
library(kknn)

tidymodels_prefer()
```

# Introduction

## Data: Ask Mattrab Note Views

The data for this analysis includes the 'Notes' table of Ask Mattrab database. The columns (reduced/modified) in the table are:

-   `id`: Primary key: an unique identifier of the note.
-   `title`: The title of the note. User reads before opening the note.
-   `body`: The number of characters that the content of the note has. Didn't include content because I can't risk leaking the note's content and it's not statistically significant for this analysis.
-   `created_at`: Date when note was created.
-   `updated_at`: Date when note was last modified.
-   `user_id` : User id (foreign key) of the creator of note.
-   `image_file_name`: Thumbnail name.
-   `image_content_type`: Thumbnail type.
-   `image_file_size`: Thumbnail size.
-   `image_updated_at`: Date when the thumbnail was last updated.
-   `view`: Total count that the note was opened and read.
-   `is_verified`: Boolean if the note is verified to be published or not.
-   `status`: If the note is draft or published.
-   `grade_id`: Grade id (foreign key) denoting which grade the note belong to.
-   `subject_id`: Subject id (foreign key) denoting which subject the note belong to.
-   `chapter_id`: Chapter id (foreign key) denoting which chapter the note belong to.
-   `position`: If the note is visible in the Ask Mattrab library or not.

Processing 2,083 records, my ultimate goal is to find best model to predict `view` from the other features.

# Loading & Cleaning the Data

The data are contained in notes_export.csv. Load the data.

```{r, message=FALSE}
notes <- read_csv("notes_export.csv")
head(notes)
summary(notes)
```

Removing the variables/features that are not very useful. These are thumbnail details, bodypdf details. creation details, last update details, etc.

```{r}

notes <- notes |>
  select(-id, -created_at, -updated_at, -image_file_name, - image_content_type, -image_file_size, -image_updated_at, -ecategory, -egrade, -chapter_id, -feedback, -verifier_id, -bodypdf_file_name, -bodypdf_content_type, -bodypdf_file_size, -bodypdf_updated_at, -topic_id)

#check the columns again
str(notes)
```

# Mutating Data

Let's mutate some data types and data values to make it friendly for the analysis. Also, let's replace any null values.

```{r}
# replacing the null cells
notes <- notes |>
  mutate(position = ifelse(is.na(position), "hidden", position), 
         grade_id = ifelse(is.na(grade_id), 12, grade_id), 
         subject_id = ifelse(is.na(subject_id), 1, subject_id),
  )

# mutating the types
notes <- notes |>
  mutate(
    title = nchar(title),
    user_id = as.factor(user_id),
    is_verified = as.factor(is_verified),
    status = factor(status, 
                    levels = c("draft", "published"),
                    ordered = TRUE),
    subject_id = as.factor(subject_id),
    grade_id = as.factor(grade_id),
    position = factor(position, 
                    levels = c("hidden", "shown"),
                    ordered = TRUE)
  )

colSums(is.na(notes)) 
```

# Data Splitting & Pre-processing

Let's split the data into training and test sets including 65% of records in the training set and stratifying the data by grade_id (because I'm interested to see which grade notes are getting more views.

```{r}
set.seed(427)

notes_split <- initial_split(notes, prop = 0.65, strata = grade_id)

notes_train <- training(notes_split)
notes_test  <- testing(notes_split)

notes_test
notes_train
```

## Recipies

Linear model recipes:

```{r}

# Simple Mean and Mode Imputation
lm_recipe1 <- recipe(view ~ ., data = notes_train) |>
  # Mean (average) imputation for numeric features
  step_impute_mean(all_numeric_predictors()) |>
  # Mode (most repeated) imputation for categorical features
  step_impute_mode(all_nominal_predictors()) |>
  # Normalize numeric predictors
  step_normalize(all_numeric_predictors()) |>
  step_unknown(all_nominal(), -all_outcomes()) |>
  # Convert categorical variables to dummy variables
  step_dummy(all_nominal_predictors())

lm_recipe2 <- recipe(view ~ ., data = notes_train) |>
  # Median imputation
  step_impute_median(all_numeric_predictors()) |>
  # Lump rare categories into "Rare"
  step_other(all_nominal_predictors(), threshold = 0.05, other = "Rare") |>
  step_unknown(all_nominal(), -all_outcomes()) |>
  step_dummy(all_nominal(), -all_outcomes()) |>
  # Remove linear combinations of variables
  step_lincomb(all_predictors()) |>
  step_center(all_numeric_predictors()) |>
  # Standardize numeric variables
  step_scale(all_numeric_predictors())

# KNN Imputation
lm_recipe3 <- recipe(view ~ ., data = notes_train) |>
  step_nzv(all_predictors()) |>
  # KNN-based imputation
  step_impute_knn(all_numeric_predictors(), neighbors = 5) |>
  # Mode imputation for categorical features
  step_impute_mode(all_nominal_predictors()) |>
  step_unknown(all_nominal(), -all_outcomes()) |>
  # Convert categorical variables to dummy variables
  step_dummy(all_nominal_predictors()) |>
  # Remove collinear variables
  step_lincomb(all_predictors()) |>
  step_nzv(all_predictors()) |> 
  step_normalize(all_numeric_predictors())

```

Knn model recipes:

```{r}

knn_recipe1 <- recipe(view ~ ., data = notes_train) |>
  step_impute_knn(all_numeric_predictors(), neighbors = 5) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_unknown(all_nominal(), -all_outcomes()) |>
  step_dummy(all_nominal_predictors())

knn_recipe2 <- recipe(view ~ ., data = notes_train) |>
  step_impute_median(all_numeric_predictors()) |>
  step_other(all_nominal_predictors(), threshold = 0.05, other = "Rare") |>
  step_scale(all_numeric_predictors()) |>
  step_unknown(all_nominal(), -all_outcomes()) |>
  step_dummy(all_nominal_predictors())


knn_recipe3 <- recipe(view ~ ., data = notes_train) |>
  step_nzv(all_predictors()) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_unknown(all_nominal_predictors()) |>
  step_other(all_nominal_predictors(), threshold = 0.01, other = "Rare") |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_corr(all_numeric_predictors(), threshold = 0.75) |>
  step_lincomb(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())
  
```

# Model-Fitting & Evaluation

Lets create the `workflow_set` that contains 12 different workflows: 3 linear regression workflows, and 9 knn workflows.

```{r}
lm_model <- linear_reg() |>
  set_engine('lm')

knn5_model <- nearest_neighbor(neighbors = 5) |>
  set_engine("kknn") |>
  set_mode("regression")
knn10_model <- nearest_neighbor(neighbors = 10) |>
  set_engine("kknn") |>
  set_mode("regression")
knn50_model <- nearest_neighbor(neighbors = 50) |>
  set_engine("kknn") |>
  set_mode("regression")

lm_models <- list(
  lm_model = lm_model
)
knn_models <- list(
  knn5 = knn5_model,
  knn10 = knn10_model,
  knn50 = knn50_model
)

lm_preprocessors <- list(
  lm_knn_impute = lm_recipe3,
  lm_mean_impute = lm_recipe1,
  lm_median_imput = lm_recipe2
)
knn_preprocessors <- list(
  knn_knn_impute = knn_recipe1,
  knn_mean_impute = knn_recipe3,
  knn_median_imput = knn_recipe2
)

knn_models <- workflow_set(knn_preprocessors, knn_models, cross = TRUE)
lm_models <-  workflow_set(lm_preprocessors, lm_models, cross = TRUE)

all_models <- lm_models |> 
  bind_rows(knn_models)
  
all_models

notes_folds <- vfold_cv(notes_train, v = 5, repeats = 5)
notes_metrics <- metric_set(rmse, rsq)

all_fits <- all_models |> 
  workflow_map("fit_resamples",
               resamples = notes_folds,
               metrics = notes_metrics)

autoplot(all_fits,  metric = "rsq") + 
  geom_text_repel(aes(label = wflow_id))

autoplot(all_fits,  metric = "rmse") + 
  geom_text_repel(aes(label = wflow_id))
```

Looking at both rsq and rmse graph, **knn_knn_impute_knn50** appears to be the best model to predict the views count of my notes based on other features.

Let's re-fit the best model on the whole training set and estimate the error metrics on the test set.

```{r}
best_workflow <- workflow() |>
  add_recipe(knn_recipe1) |>
  add_model(knn50_model)

best_fit <- best_workflow |>
  fit(data = notes_train)
best_fit

predictions <- predict(best_fit, new_data = notes_test)

results <- notes_test |>
  bind_cols(predictions)

metrics <- metric_set(rmse, rsq)
error_metrics <- results |>
  metrics(truth = view, estimate = .pred)

error_metrics
```

The best **rsq** that I obtained is 0.2085752 which is not super good. Selecting only specific variables/features can make the model better. If you provide me an interview opportunity, I will tell you how. :)
